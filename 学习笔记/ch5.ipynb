{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9302de",
   "metadata": {},
   "source": [
    "### 误差反向传播法\n",
    "- 使用**计算图**可以通过反向传播高效计算导数。\n",
    "- 比较数值微分和误差反向传播法的结果，可以确认误差反向传播法的实现是否正确。\n",
    "- 计算图的反向传播从右往左传播信号，反向传播的计算顺序是：先将节点的输入信号乘以节点的局部导数(偏导数)，然后再传递给下一个节点。\n",
    "- **加法节点**的反向传播只是将输入信号输出到下一个节点。\n",
    "  <div align=\"center\"><img src=\"data_mk/mk-2025-11-03-23-08-48.png\" width=\"75%\" style=\"margin: 20px 0;\"></div>\n",
    "- **乘法节点**的反向传播会乘以输入信号的翻转值。<mark>实现乘法节点的反向传播时，要保存正向传播的输入信号。</mark>\n",
    "  <div align=\"center\"><img src=\"data_mk/mk-2025-11-03-23-15-39.png\" width=\"75%\" style=\"margin: 20px 0;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22690f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 乘法层（Multiplication Layer）\n",
    "# ==========================================\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        # 保存正向传播时输入的值（用于反向传播）\n",
    "        # 在计算图中，一个节点的输出往往依赖于输入的值，\n",
    "        # 因此在反向传播阶段需要将这些输入值重新取出使用。\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        前向传播 (Forward propagation)\n",
    "        输入: x, y\n",
    "        输出: out = x * y\n",
    "        \"\"\"\n",
    "        # 将输入保存下来，方便反向传播使用\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        # 乘法层的输出\n",
    "        out = x * y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向传播 (Backward propagation)\n",
    "        dout: 来自上游层的梯度 (∂L/∂out)\n",
    "\n",
    "        计算目标：\n",
    "            根据链式法则：\n",
    "                dx = ∂L/∂x = ∂L/∂out * ∂out/∂x = dout * y\n",
    "                dy = ∂L/∂y = ∂L/∂out * ∂out/∂y = dout * x\n",
    "        \"\"\"\n",
    "        dx = dout * self.y  # 反向传播到 x 的梯度\n",
    "        dy = dout * self.x  # 反向传播到 y 的梯度\n",
    "        return dx, dy\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 加法层（Addition Layer）\n",
    "# ==========================================\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        # 加法层没有需要保存的参数，因为加法的导数恒为 1\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        前向传播:\n",
    "        输入: x, y\n",
    "        输出: out = x + y\n",
    "        \"\"\"\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向传播:\n",
    "        dout: 来自上游层的梯度 (∂L/∂out)\n",
    "\n",
    "        根据链式法则：\n",
    "            out = x + y\n",
    "            ∂out/∂x = 1, ∂out/∂y = 1\n",
    "        所以：\n",
    "            dx = dout * 1 = dout\n",
    "            dy = dout * 1 = dout\n",
    "        \"\"\"\n",
    "        dx = dout  # 对 x 的梯度\n",
    "        dy = dout  # 对 y 的梯度\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6810cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# ReLU层（Rectified Linear Unit 激活层）\n",
    "# ==========================================\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        # mask 是一个布尔数组，用来记录哪些神经元在前向传播中被“屏蔽”（即输入<=0）\n",
    "        # 在反向传播时，被屏蔽的神经元梯度会被置为0，不参与更新。\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播 (Forward propagation)\n",
    "        ReLU函数定义为：f(x) = max(0, x)\n",
    "        作用：将输入中小于等于0的值截断为0，大于0的部分保持不变。\n",
    "\n",
    "        ReLU的意义：\n",
    "            - 引入非线性，使神经网络具备逼近复杂函数的能力；\n",
    "            - 相比 Sigmoid/Tanh，ReLU 不会饱和，计算简单，收敛更快。\n",
    "        \"\"\"\n",
    "        # mask 标记出小于等于0的位置\n",
    "        self.mask = (x <= 0)\n",
    "\n",
    "        # 将输入复制一份，避免原数据被修改\n",
    "        out = x.copy()\n",
    "\n",
    "        # 将 mask 为 True 的位置（即 x<=0）置为 0\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向传播 (Backward propagation)\n",
    "        dout: 来自上游层的梯度 (∂L/∂out)\n",
    "\n",
    "        对于 ReLU:\n",
    "            f(x) = max(0, x)\n",
    "            当 x > 0 时，∂f/∂x = 1\n",
    "            当 x <= 0 时，∂f/∂x = 0\n",
    "\n",
    "        因此：\n",
    "            dx = dout * (x > 0)\n",
    "        \"\"\"\n",
    "        # 对 mask 中为 True 的（即 x<=0）梯度置 0\n",
    "        dout[self.mask] = 0\n",
    "\n",
    "        # 反向传播后的梯度（dx = dout * mask）\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Sigmoid层（Sigmoid 激活层）\n",
    "# ==========================================\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        # 保存前向传播的输出值，在反向传播时使用\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播:\n",
    "        Sigmoid函数定义为：\n",
    "            f(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "        特性：\n",
    "            - 将输入“压缩”到 (0, 1) 区间；\n",
    "            - 常用于输出概率或二分类任务；\n",
    "            - 在深层网络中容易出现梯度消失问题（因为导数在饱和区接近0）。\n",
    "        \"\"\"\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向传播:\n",
    "        Sigmoid函数的导数为：\n",
    "            f'(x) = f(x) * (1 - f(x))\n",
    "\n",
    "        利用链式法则：\n",
    "            dx = dout * f'(x)\n",
    "               = dout * (1.0 - self.out) * self.out\n",
    "        \"\"\"\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87197702",
   "metadata": {},
   "source": [
    "### Sigmoid层计算图\n",
    "<div align=\"center\">   <img src=\"data_mk/mk-2025-11-04-20-08-59.png\" width=\"75%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial y}y^2\\exp(-x)\n",
    "&= \\frac{\\partial L}{\\partial y}\\frac{1}{(1+\\exp(-x))^2}\\exp(-x) \\\\\n",
    "&= \\frac{\\partial L}{\\partial y}\\frac{1}{1+\\exp(-x)}\\frac{\\exp(-x)}{1+\\exp(-x)} \\\\\n",
    "&=\\frac{\\partial L}{\\partial y}y(1-y)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6febf735",
   "metadata": {},
   "source": [
    "### 以矩阵为对象的反向传播\n",
    "<div align=\"center\">   <img src=\"data_mk/mk-2025-11-04-20-36-52.png\" width=\"75%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "$$\n",
    "\\bm{X} = (x_0,x_1,...,x_n)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\bm{X}} = (\\frac{\\partial L}{\\partial x_0},\\frac{\\partial L}{\\partial x_1},...,\\frac{\\partial L}{\\partial x_n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c0ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Affine:\n",
    "    \"\"\"\n",
    "    Affine层（全连接层 / 仿射层）\n",
    "    作用：实现 y = xW + b 的前向传播，并能计算反向传播的梯度。\n",
    "    该层通常位于神经网络的隐藏层或输出层中，用于线性变换输入特征。\n",
    "    \"\"\"\n",
    "    def __init__(self, W, b):\n",
    "        # 权重参数 W: (输入维度, 输出维度)\n",
    "        # 偏置参数 b: (输出维度,)\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        # 记录前向传播的输入（用于反向传播）\n",
    "        self.x = None\n",
    "        \n",
    "        # 保存输入的原始形状（例如卷积层输出是4维的，需要还原）\n",
    "        self.oringinal_x_shape = None\n",
    "        \n",
    "        # 存放反向传播时计算得到的梯度\n",
    "        self.dW = None  # 对权重的梯度\n",
    "        self.db = None  # 对偏置的梯度\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播：计算仿射变换输出\n",
    "        y = xW + b\n",
    "        \"\"\"\n",
    "        # -----------------【语法讲解】-----------------\n",
    "        # x.reshape(x.shape[0], -1)\n",
    "        # 含义：将输入x（可能是多维数据，如图像）拉平成二维矩阵。\n",
    "        #       例如输入(batch_size, channels, height, width)\n",
    "        #       -> 输出(batch_size, channels*height*width)\n",
    "        # reshape 中 -1 表示“自动推导”剩余维度大小\n",
    "        # 这样做是为了与权重矩阵 W (二维) 相乘。\n",
    "        # ---------------------------------------------\n",
    "        self.oringinal_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        \n",
    "        # 仿射变换：矩阵乘法 + 偏置\n",
    "        # 对应深度学习中的线性层 y = xW + b\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向传播：\n",
    "        已知上游梯度 dout，计算该层输入 x、权重 W、偏置 b 的梯度\n",
    "        \"\"\"\n",
    "        # dout: (batch_size, 输出维度)\n",
    "        # W: (输入维度, 输出维度)\n",
    "\n",
    "        # 输入梯度 dx = dout · W^T\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "\n",
    "        # 权重梯度 dW = x^T · dout\n",
    "        # 这对应损失函数对权重参数的偏导\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "\n",
    "        # -----------------【语法讲解】-----------------\n",
    "        # np.sum(dout, axis=0)\n",
    "        # 含义：对每个输出维度方向求和，得到每个偏置的梯度。\n",
    "        # 因为偏置 b 是对所有样本共享的，所以要把 batch 内的梯度加总。\n",
    "        # 举例：若 dout.shape = (100, 10)\n",
    "        # 则 db.shape = (10,)\n",
    "        # ---------------------------------------------\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        # -----------------【语法讲解】-----------------\n",
    "        # dx.reshape(*self.oringinal_x_shape)\n",
    "        # 含义：将 dx 从二维矩阵还原为输入的原始形状。\n",
    "        # 星号 * 是“解包”操作符，\n",
    "        # 比如 original_x_shape=(100,1,28,28)，\n",
    "        # 等价于 dx.reshape(100,1,28,28)\n",
    "        # 这样便可在卷积层与全连接层之间无缝衔接。\n",
    "        # ---------------------------------------------\n",
    "        dx = dx.reshape(*self.oringinal_x_shape)\n",
    "\n",
    "        # 返回输入的梯度\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38484d26",
   "metadata": {},
   "source": [
    "### Softmax层\n",
    "一、Softmax 层的作用\n",
    "\n",
    "Softmax 层常用在**分类任务的输出层**，\n",
    "它的作用是将网络输出的“原始得分”（logits）转化为**概率分布**，\n",
    "使得每个类别的输出都在 ([0,1]) 之间，并且所有类别的概率之和为 1。\n",
    "\n",
    "二、数学表达式\n",
    "\n",
    "设网络的最后一层输出为一个向量：\n",
    "$$\n",
    "\\mathbf{a} = [a_1, a_2, ..., a_K]\n",
    "$$\n",
    "其中$K$是类别数（比如 10 分类任务中 $K=10$）。\n",
    "\n",
    "Softmax 的定义为：\n",
    "$$\n",
    "y_i = \\frac{\\exp(a_i)}{\\sum_{j=1}^{K} \\exp(a_j)}\n",
    "$$\n",
    "\n",
    "所以输出 $y_i$ 表示第 $i$ 类的预测概率。\n",
    "\n",
    "三、为什么要用 Softmax？\n",
    "\n",
    "网络输出的$a_i$ 一般是任意实数（可能为负数），\n",
    "直接拿来代表概率显然不合理。\n",
    "\n",
    "Softmax 有两个关键特性：\n",
    "\n",
    "1. **非负性**：$\\exp(a_i) > 0$，保证输出全为正；\n",
    "2. **归一化**：分母求和后让所有概率加起来等于 1。\n",
    "\n",
    "因此 Softmax 的输出就是一个概率分布，非常适合分类问题。\n",
    "\n",
    "\n",
    "四、与交叉熵误差（Cross Entropy Error）的结合\n",
    "\n",
    "如果只用 Softmax，还不能度量预测与真实标签的差距。\n",
    "因此在训练中，我们把它和交叉熵损失函数结合：\n",
    "\n",
    "$$\n",
    "L = - \\sum_{i} t_i \\log(y_i)\n",
    "$$\n",
    "其中：\n",
    "\n",
    "* $t_i$ 是真实标签（通常是 one-hot 向量）\n",
    "* $y_i$ 是 Softmax 输出的预测概率\n",
    "<div align=\"center\">\n",
    "\n",
    "| 类别 | One-hot 表示   |\n",
    "| -- | ------------ |\n",
    "| 0  | [1, 0, 0, 0] |\n",
    "| 1  | [0, 1, 0, 0] |\n",
    "| 2  | [0, 0, 1, 0] |\n",
    "| 3  | [0, 0, 0, 1] |\n",
    "\n",
    "</div>\n",
    "这个公式的含义：\n",
    "\n",
    "> 当真实类别 $t_i=1$ 时，损失只取该类对应的 $-\\log(y_i)$。\n",
    "\n",
    "所以：\n",
    "\n",
    "* 如果预测概率接近 1，损失趋近于 0；\n",
    "* 如果预测概率接近 0，损失趋近于无穷大；\n",
    "  → 网络会被“惩罚”得更厉害。\n",
    "\n",
    "五、Softmax-with-Loss 层的反向传播\n",
    "\n",
    "这层在反向传播时有个非常重要的性质（公式推导后可以简化）：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_i} = y_i - t_i\n",
    "$$\n",
    "\n",
    "也就是说，**Softmax + Cross Entropy 的组合可以直接得到梯度**，\n",
    "从而避免显式计算 Softmax 的复杂导数，\n",
    "这也是它们总是一起实现的原因。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d516dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Softmax 函数：将神经网络的输出（logits）转换为概率分布\n",
    "# ============================================================\n",
    "def softmax(x):\n",
    "    # ------------------------ 多样本（二维输入）情况 ------------------------\n",
    "    # x.shape = (batch_size, 类别数)\n",
    "    if x.ndim == 2:\n",
    "        # 为了方便按列计算 softmax，先转置\n",
    "        x = x.T\n",
    "        \n",
    "        # 数值稳定化处理（防止指数运算时溢出）\n",
    "        # 减去每一列的最大值，不影响相对概率，但能防止 exp() 溢出为无穷大\n",
    "        x = x - np.max(x, axis=0)\n",
    "\n",
    "        # softmax 计算公式：exp(x_i) / Σ exp(x_j)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        # 最后再转置回来，保持与输入形状一致\n",
    "        return y.T \n",
    "\n",
    "    # ------------------------ 单样本（一维输入）情况 ------------------------\n",
    "    # 同样进行溢出对策（数值稳定化）\n",
    "    x = x - np.max(x)\n",
    "    # softmax 函数输出：每个元素代表该类别的预测概率\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 交叉熵误差（Cross Entropy Error）\n",
    "# 用于衡量预测概率分布 y 与真实分布 t 的差异\n",
    "# ============================================================\n",
    "def cross_entropy_error(y, t):\n",
    "    # ------------------------ 统一为二维处理 ------------------------\n",
    "    # 若输入为单一样本（一维），则转换为二维以便批量计算\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)   # t: (类别数,) -> (1, 类别数)\n",
    "        y = y.reshape(1, y.size)   # y: (类别数,) -> (1, 类别数)\n",
    "        \n",
    "    # ------------------------ One-hot 标签处理 ------------------------\n",
    "    # 若 t 与 y 形状相同（说明 t 是 one-hot 向量）\n",
    "    # 则取 argmax 得到每个样本真实类别的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    # ------------------------ 损失计算 ------------------------\n",
    "    batch_size = y.shape[0]\n",
    "    # 取出每个样本中真实类别对应的预测概率 y[i, t[i]]\n",
    "    # 加上一个极小值 1e-7 防止 log(0) 导致数值错误\n",
    "    # 最后取负号并对整个 batch 求平均\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Softmax-with-Loss 层\n",
    "# 结合了 Softmax 激活函数 与 交叉熵损失函数\n",
    "# 前向传播：计算概率与损失\n",
    "# 反向传播：计算梯度（简化为 y - t）\n",
    "# ============================================================\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None  # 损失值（标量）\n",
    "        self.y = None     # softmax 输出（概率分布）\n",
    "        self.t = None     # 真实标签（监督数据）\n",
    "\n",
    "    # ------------------------ 前向传播 ------------------------\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: 神经网络的输出（logits），形状 (batch_size, 类别数)\n",
    "        t: 监督标签，可以是 one-hot 向量或类别索引\n",
    "        \"\"\"\n",
    "        self.t = t\n",
    "        # 计算 softmax 概率分布\n",
    "        self.y = softmax(x)\n",
    "        # 计算交叉熵损失\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    # ------------------------ 反向传播 ------------------------\n",
    "    def backward(self, dout=1):\n",
    "        \"\"\"\n",
    "        dout: 上游传来的梯度（通常为1，因为 loss 是标量）\n",
    "        返回：dx —— 当前层输入的梯度，用于传给前一层\n",
    "        \"\"\"\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        # ---------------- One-hot 标签情况 ----------------\n",
    "        # 结合 Softmax 与 CrossEntropy 的反向传播有个重要结论：\n",
    "        #   ∂L/∂x = (y - t) / batch_size\n",
    "        # 这是由于两者结合后梯度极大简化，避免了显式计算 softmax 的复杂导数\n",
    "        \"\"\"\n",
    "        self.t.size == self.y.size说明监督标签 t 是 one-hot 向量（和 y 的形状一样）。\n",
    "        例如：\n",
    "        y = [[0.1, 0.7, 0.2],   # 第1个样本的预测概率\n",
    "             [0.8, 0.1, 0.1]]   # 第2个样本的预测概率\n",
    "        t = [[0, 1, 0],         # 第1个样本真实类别为 1\n",
    "             [1, 0, 0]]         # 第2个样本真实类别为 0\n",
    "        \"\"\"\n",
    "        if self.t.size == self.y.size:\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        # ---------------- 标签为类别索引情况 ----------------\n",
    "        # 若 t 不是 one-hot（例如 t = [2, 0, 1]）\n",
    "        # 则需要手动构造与之对应的梯度\n",
    "        else:\n",
    "            dx = self.y.copy()  # 复制预测概率\n",
    "            # 对于真实类别 t[i]，其梯度为 (y_i - 1)\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        # 返回输入梯度（供前层使用）\n",
    "        return dx\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
