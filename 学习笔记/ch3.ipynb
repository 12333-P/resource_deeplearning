{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d7414d",
   "metadata": {},
   "source": [
    "# 激活函数的作用\n",
    "1. **引入非线性因素**：这是激活函数最核心的作用。如果没有激活函数，无论神经网络有多少层，其最终的输出都只能是输入的线性组合（等价于一个单层感知机），无法拟合现实世界中的复杂非线性关系 。激活函数通过其非线性特性，使得神经网络能够学习和表示极其复杂的模式，如图像识别、语音处理等。\n",
    "2. **决定神经元的激活状态**：激活函数$h(a)$根据输入信号的加权和$a$，来决定一个神经元是否被“激活”以及激活的程度，从而控制信息在网络中的流动。\n",
    "$$a = b+\\omega_1x_1+\\omega_2x_2$$ $$y =h(a)$$\n",
    "### 常见的激活函数\n",
    "\n",
    "以下是一些在深度学习中非常常见的激活函数：\n",
    "\n",
    "| 函数类型 | 函数名称 | 数学公式 / 特性 | 输出范围 | 常见应用场景 |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **阶跃函数** | 感知机激活函数 |$h(x) = \\begin{cases} 0 & x \\leq 0 \\\\ 1 & x > 0 \\end{cases}$  | {0, 1} | 单层感知机  |\n",
    "| **S型函数** | **Sigmoid** | $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $  | (0, 1) | 二分类问题的输出层（将输出解释为概率） |\n",
    "| | **Tanh** (双曲正切) | $ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $  | (-1, 1) | 循环神经网络（RNN）的隐藏层  |\n",
    "| **线性整流单元及其变种** | **ReLU** | $ \\text{ReLU}(x) = \\max(0, x) $  | [0, +∞) | **最常用**，广泛应用于卷积神经网络（CNN）和多层感知机的隐藏层  |\n",
    "| | **Leaky ReLU** | $ \\text{LReLU}(x) = \\begin{cases} x & x \\geq 0 \\\\ \\alpha x & x < 0 \\end{cases} $（α为小的正常数，如0.01） | (-∞, +∞) | 旨在解决ReLU的“神经元死亡”问题  |\n",
    "| | **ELU** (指数线性单元) | $ \\text{ELU}(x) = \\begin{cases} x & x \\geq 0 \\\\ \\alpha(e^x - 1) & x < 0 \\end{cases} $  | (-α, +∞) | 同样旨在解决ReLU的问题，通常能获得比ReLU更快的收敛速度  |\n",
    "| **多分类函数** | **Softmax** | $ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}} $  | (0, 1) | **多分类问题**的输出层，将输出转化为概率分布（所有输出之和为1） |\n",
    "\n",
    "在实际应用中，激活函数的选择有一些经验法则可循：\n",
    "-   **隐藏层**：**ReLU** 及其变体（如Leaky ReLU, ELU）由于其良好的表现和计算效率，通常是首选 。\n",
    "-   **输出层**：\n",
    "    -   **二分类问题**：通常使用 **Sigmoid** 函数，将输出解释为属于正类的概率 。\n",
    "    -   **多分类问题**：使用 **Softmax** 函数，输出每个类别的概率 。\n",
    "    -   **回归问题**（如预测价格、温度）：通常使用**恒等函数**（即不做变换，直接输出）。\n",
    "### 一些细节\n",
    "- 激活函数不能使用线性函数，否则加深神经网络的层数就没有意义了。\n",
    "$$h(x) = cx$$ $$y(x) = h(h(h(x)))$$ $$y(x) = ax(a=c^3)$$\n",
    "- 激活函数平滑性的作用\n",
    "<div align=\"center\">\n",
    "\n",
    "| 特性 | Sigmoid函数 (平滑) | 阶跃函数 (非平滑) |\n",
    "| :--- | :--- | :--- |\n",
    "| **可微性** | 处处可导，导数连续变化 | 在零点不可导，导数无定义 |\n",
    "| **梯度下降** | 支持，可平滑收敛 | 无法直接应用 |\n",
    "| **梯度信号** | 提供连续、细致的权重更新方向 | 无法提供有效的更新方向 |\n",
    "| **优化稳定性** | 优化过程相对平稳 | 优化过程不稳定 |\n",
    "\n",
    "</div>\n",
    "\n",
    "- 对于分类问题，输出层的神经元一般设置为其类别的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4792c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    \"\"\"\n",
    "    阶跃函数实现\n",
    "    当输入大于0时返回1，否则返回0\n",
    "    \n",
    "    参数:\n",
    "    x: 输入值，可以是标量或numpy数组\n",
    "    \n",
    "    返回:\n",
    "    与x形状相同的数组，元素为0或1\n",
    "    \"\"\"\n",
    "    # 步骤1: x > 0 进行元素级比较，生成布尔数组\n",
    "    # 步骤2: np.array() 将布尔数组转换为指定数据类型的数组\n",
    "    # 步骤3: dtype=np.int 确保输出为整数类型(0或1)\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))   \n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU (Rectified Linear Unit) 激活函数\n",
    "    对输入进行逐元素操作，将所有负值置为0，正值保持不变\n",
    "    \n",
    "    参数:\n",
    "    x: 输入值，可以是标量、列表或numpy数组\n",
    "    \n",
    "    返回:\n",
    "    与x形状相同的数组，负值被替换为0，正值保持不变\n",
    "    \"\"\"\n",
    "    # 使用np.maximum进行逐元素比较\n",
    "    # 第一个参数0表示基准值，第二个参数x是输入数据\n",
    "    # 函数会比较每个位置的x值与0的大小，返回较大者\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    计算Softmax函数，将输入转换为概率分布\n",
    "    \n",
    "    Args:\n",
    "        x: 输入数组，可以是一维或二维\n",
    "        \n",
    "    Returns:\n",
    "        Softmax概率分布，所有元素之和为1，范围在[0,1]之间\n",
    "    \"\"\"\n",
    "    # 处理二维数组情况（批量处理）\n",
    "    if x.ndim == 2:\n",
    "        x = x.T  # 转置以便按列处理\n",
    "        x = x - np.max(x, axis=0)  # 每列减去最大值，防止数值溢出\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)  # 计算Softmax\n",
    "        return y.T  # 转置回原始形状\n",
    "    \n",
    "    # 处理一维数组情况\n",
    "    x = x - np.max(x)  # 减去最大值，防止指数运算溢出\n",
    "    return np.exp(x) / np.sum(np.exp(x))  # 计算Softmax概率分布\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f64886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"\n",
    "    加载MNIST数据集用于测试\n",
    "    \n",
    "    Returns:\n",
    "        x_test: 测试集图像数据，已进行归一化处理\n",
    "        t_test: 测试集标签数据，非one-hot编码形式\n",
    "    \"\"\"\n",
    "    # 加载MNIST数据集，参数说明：\n",
    "    # normalize=True: 对图像数据进行归一化（0.0~1.0）\n",
    "    # flatten=True: 将28×28的图像平展为784维的一维数组\n",
    "    # one_hot_label=False: 标签不使用one-hot编码，而是直接使用数字0-9\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "\n",
    "def init_network():\n",
    "    \"\"\"\n",
    "    初始化已训练好的神经网络模型\n",
    "    \n",
    "    Returns:\n",
    "        network: 包含已训练权重和偏置的字典\n",
    "    \"\"\"\n",
    "    # 从pickle文件中加载预训练的神经网络参数\n",
    "    # 文件包含已训练好的权重(W1, W2, W3)和偏置(b1, b2, b3)\n",
    "    with open(\"sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "\n",
    "def predict(network, x):\n",
    "    \"\"\"\n",
    "    使用神经网络进行前向传播预测\n",
    "    \n",
    "    Args:\n",
    "        network: 包含权重和偏置的神经网络参数\n",
    "        x: 输入数据（单个或多个样本）\n",
    "    \n",
    "    Returns:\n",
    "        y: 预测结果的概率分布\n",
    "    \"\"\"\n",
    "    # 从网络参数字典中提取各层的权重和偏置\n",
    "    w1, w2, w3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    # 第一层前向传播：仿射变换 + Sigmoid激活\n",
    "    a1 = np.dot(x, w1) + b1  # 仿射变换\n",
    "    z1 = sigmoid(a1)         # 通过Sigmoid激活函数\n",
    "    \n",
    "    # 第二层前向传播：仿射变换 + Sigmoid激活  \n",
    "    a2 = np.dot(z1, w2) + b2  # 第二层仿射变换\n",
    "    z2 = sigmoid(a2)         # 通过Sigmoid激活函数\n",
    "    \n",
    "    # 输出层前向传播：仿射变换 + Softmax归一化\n",
    "    a3 = np.dot(z2, w3) + b3  # 输出层仿射变换\n",
    "    y = softmax(a3)          # 通过Softmax函数转换为概率分布\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "# 获取测试数据和初始化网络\n",
    "x, t = get_data()           # x:测试图像, t:对应标签\n",
    "network = init_network()    # 加载预训练网络参数\n",
    "\n",
    "# 批处理预测参数设置\n",
    "batch_size = 100  # 批处理大小，每次处理100个样本以提高效率\n",
    "accuracy_cnt = 0  # 正确分类计数\n",
    "\n",
    "# 使用批处理进行预测，减少内存占用并提高计算效率\n",
    "for i in range(0, len(x), batch_size):\n",
    "    # 获取当前批次的数据\n",
    "    x_batch = x[i:i+batch_size]  # 当前批次的输入数据（100个样本）\n",
    "    y_batch = predict(network, x_batch)  # 对批次数据进行预测\n",
    "    \n",
    "    # 获取预测结果：取概率最大值的索引作为预测数字\n",
    "    p = np.argmax(y_batch, axis=1)  # 沿第一个轴（每行）取最大值索引\n",
    "    \n",
    "    # 统计本批次中正确预测的数量\n",
    "    # p == t[i:i+batch_size] 会生成布尔数组，True表示预测正确\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "\n",
    "# 计算并输出整体准确率\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
