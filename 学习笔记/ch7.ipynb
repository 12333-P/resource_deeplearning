{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8f9936",
   "metadata": {},
   "source": [
    "### 整体结构\n",
    "- 全连接神经网络（Fully-connected Neural Network, FCN），相邻层的所有神经元之间都有连接，这称为**全连接（fully-connected）**。另外，我们用Affine层实现了全连接层。\n",
    "<div align=\"center\">   <img src=\"data_mk/mk-2025-11-06-20-52-59.png\" width=\"50%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    " > 全连接的神经网络中，Affine层后面跟着激活函数ReLU层（或者Sigmoid层）。这里堆叠了4层“Affine-ReLU”组合，然后第5层是Affine层，最后由Softmax层输出最终结果（概率）。\n",
    "- 卷积神经网络（Convolutional Neural Network, CNN）中新增了Convolution层和Pooling层。CNN的层的连接顺序是“**Convolution - ReLU -（Pooling）** ”（Pooling层有时会被省略）。这可以理解为之前的“Affine - ReLU”连接被替换成了“Convolution - ReLU -（Pooling）”连接。\n",
    "<div align=\"center\">   <img src=\"data_mk/mk-2025-11-06-20-57-11.png\" width=\"50%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "> 还需要注意的是，靠近输出的层中使用了之前的“Affine - ReLU”组合。此外，最后的输出层中使用了之前的“Affine - Softmax”组合。\n",
    "<div align=\"center\">\n",
    "\n",
    "| 位置     | 使用结构                  | 目的              |\n",
    "| ------ | --------------------- | --------------- |\n",
    "| 卷积层前端  | Conv - ReLU - Pooling | 提取局部特征          |\n",
    "| 靠近输出的层 | Affine - ReLU         | 综合全局特征，形成高层语义表示 |\n",
    "| 最终输出层  | Affine - Softmax      | 输出类别概率，完成分类     |\n",
    "\n",
    "</div>\n",
    "\n",
    "- 对比\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| 特性     | 全连接网络（FCN）      | 卷积网络（CNN）      |\n",
    "| ------ | --------------- | -------------- |\n",
    "| 参数连接方式 | 每个神经元与下一层所有节点相连 | 局部连接（只连接局部区域）  |\n",
    "| 参数量    | 非常多             | 大幅减少（共享权重）     |\n",
    "| 特征提取   | 无法自动提取空间特征(数据被拉平为一维)      | 能自动提取图像局部与空间特征 |\n",
    "| 应用场景   | 一般数据、表格类任务      | 图像、视频、语音等高维数据  |\n",
    "\n",
    "</div>\n",
    "\n",
    "- CNN中，将卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）\n",
    "\n",
    "### 卷积层\n",
    "- **卷积运算**：对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用。将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和。然后，将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。\n",
    "> CNN中，滤波器的参数就对应之前的权重。并且，CNN中也存在偏置。\n",
    "<div align=\"center\">   <img src=\"data_mk/mk-2025-11-06-21-08-29.png\" width=\"50%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "- **填充**：在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如0等），这称为填充（padding）。\n",
    "<div align=\"center\">   <img src=\"data_mk/mk-2025-11-06-21-12-57.png\" width=\"50%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "> 如图所示，通过填充，大小为(4,4)的输入数据变成了(6,6)的形状。然后，应用大小为(3,3)的滤波器，生成了大小为(4,4)的输出数据。这个例子中将填充设成了1，不过填充的值也可以设置成2、3等任意的整数。\n",
    "> 使用填充主要是为了调整输出的大小。如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为1，导致无法再应用卷积运算。\n",
    "\n",
    "- **步幅**：应用滤波器的位置间隔称为步幅（stride）。\n",
    "<div align=\"center\">   <img src=\"data_mk/mk-2025-11-06-21-15-55.png\" width=\"40%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "- **如何计算输出大小**：假设输入大小为$(H,W)$，滤波器大小为$(FH,FW)$，输出大小为$(OH,OW)$，填充为$P$，步幅为$S$。  \n",
    "  计算公式：\n",
    "$$\n",
    "OH = \\frac{H+2P-FH}{S}+1\n",
    "$$\n",
    "$$\n",
    "OW = \\frac{W+2P-FW}{S}+1\n",
    "$$\n",
    "\n",
    "- **三维卷积运算**：通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。\n",
    "<div align=\"center\">   <img src=\"data_mk\\mk-2025-11-06-21-27-27.png\" width=\"55%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "> 输入数据和滤波器的通道数一致。\n",
    "- 结合方块思考。\n",
    "  <div align=\"center\">   <img src=\"data_mk\\mk-2025-11-06-21-29-48.png\" width=\"55%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "  如果要在通道方向上也拥有多个卷积运算的输出，该怎么做呢？为此，就需要用到多个滤波器（权重）。\n",
    "  <div align=\"center\">   <img src=\"data_mk\\mk-2025-11-06-21-30-42.png\" width=\"55%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "> 通过应用FN个滤波器，输出特征图也生成了FN个。如果将这FN个特征图汇集在一起，就得到了形状为(FN,OH,OW)的方块。将这个方块传给下一层，就是CNN的处理流。  \n",
    "> 滤波器的权重数据要按(output_channel,input_channel, height, width)的顺序书写。比如，通道数为$3$、大小为$5×5$的滤波器有$20$个时，可以写成$(20,3,5,5)$。\n",
    "<div align=\"center\">   <img src=\"data_mk\\mk-2025-11-06-21-34-27.png\" width=\"55%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "> 每个通道只有一个偏置。这里，偏置的形状是$(FN,1,1)$，滤波器的输出结果的形状是$(FN,OH,OW)$。这两个方块相加时，要对滤波器的输出结果$(FN,OH,OW)$按通道加上相同的偏置值。另外，不同形状的方块相加时，可以基于NumPy的广播功能轻松实现。\n",
    "- **批处理**：神经网络的处理中进行了将输入数据打包的批处理。为此，需要将在各层间传递的数据保存为4维数据。具体地讲，就是按(batch_num,channel,height,width)的顺序保存数据。  \n",
    "  卷积运算的处理流（批处理）：\n",
    "<div align=\"center\">   <img src=\"data_mk\\mk-2025-11-06-21-38-24.png\" width=\"55%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "### 池化层\n",
    "- **池化**是缩小高、长方向上的空间的运算。如图所示，进行将$2×2$的区域集约成$1$个元素的处理，缩小空间大小（Max池化）。\n",
    "<div align=\"center\">   <img src=\"data_mk\\mk-2025-11-06-21-40-25.png\" width=\"55%\" style=\"margin: 20px 0;\" /> </div>\n",
    "\n",
    "> 一般来说，池化的窗口大小会和步幅设定成相同的值。\n",
    "#### **池化层的特征**\n",
    "1. **没有要学习的参数**：池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。\n",
    "2. **通道数不发生变化**：经过池化运算，输入数据和输出数据的通道数不会发生变化。\n",
    "3. **对微小的位置变化具有鲁棒性（健壮）**：对微小的位置变化具有鲁棒性（健壮）。\n",
    "#### **常见的池化方式**\n",
    "<div align=\"center\">\n",
    "\n",
    "| 池化方式                               | 计算方法               | 特点             | 优点                 | 缺点         | 常见应用                          |\n",
    "| ---------------------------------- | ------------------ | -------------- | ------------------ | ---------- | ----------------------------- |\n",
    "| **Max Pooling（最大池化）**              | 从目标区域中取出**最大值**    | 保留显著特征（如边缘、纹理） | 计算简单，保留关键信息，对平移不敏感 | 可能丢失细节信息   | 图像识别、卷积神经网络主流用法               |\n",
    "| **Average Pooling（平均池化）**          | 计算目标区域中所有值的**平均值** | 平滑特征图          | 降低噪声、保持平滑过渡        | 可能削弱显著特征   | 早期CNN（如LeNet）或特征平滑任务          |\n",
    "| **Global Average Pooling（全局平均池化）** | 对整个特征图取平均值         | 将每个特征图压缩为一个数   | 无需全连接层，减少参数，防止过拟合  | 可能损失空间位置信息 | 现代轻量CNN（如GoogLeNet、ResNet）输出层 |\n",
    "| **Global Max Pooling（全局最大池化）**     | 对整个特征图取最大值         | 取出最强响应特征       | 保留最显著的全局特征         | 容易忽略次要特征   | 注意力机制、显著性检测                   |\n",
    "| **L2 Pooling（平方平均池化）**             | 对区域内平方求和再开方        | 平衡最大与平均        | 保留强特征同时抑制噪声        | 计算复杂度高     | 特殊场景（如部分视觉模型）                 |\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c49123a",
   "metadata": {},
   "source": [
    "### 基于im2col的展开\n",
    "- 如果老老实实地实现卷积运算，估计要重复好几层的for语句。这样的实现有点麻烦，而且，NumPy中存在使用for语句后处理变慢的缺点（NumPy中，访问元素时最好不要用for语句）。这里，我们不使用for语句，而是使\n",
    "用im2col这个便利的函数进行简单的实现。\n",
    "<div align=\"center\">\n",
    "\n",
    "| 项目   | 传统卷积      | 使用 `im2col` 的卷积 |\n",
    "| ---- | --------- | --------------- |\n",
    "| 运算形式 | 嵌套循环滑动窗口  | 矩阵乘法            |\n",
    "| 优化空间 | 访问不连续，难并行 | 可调用高性能 GEMM 库   |\n",
    "| 内存开销 | 小         | 较大（数据展开）        |\n",
    "| 适用场景 | 小输入、内存受限  | 大规模卷积层（GPU优化）   |\n",
    "\n",
    "</div>\n",
    "\n",
    "- im2col会把输入数据展开以适合滤波器（权重）。对于**输入数据**，将应用滤波器的区域（3维方块）横向展开为**1列**。在滤波器的应用区域重叠的情况下，使用im2col展开后，展开后的元素个数会多于原方块的元素个数。因此，使用im2col的实现存在比普通的实现消耗更多内存的缺点。使用im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为**1列**，并计算2个矩阵的乘积即可。这和全连接层的Affine 层进行的处理基本相同。\n",
    "<div align=\"center\">   <img src=\"data_mk\\mk-2025-11-06-22-35-29.png\" width=\"55%\" style=\"margin: 20px 0;\" /> </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    将输入图像数据 (batch of feature maps) 转换为二维矩阵形式，以便用矩阵乘法计算卷积。\n",
    "\n",
    "    在深度学习的卷积层（Convolution Layer）中，我们通常希望：\n",
    "      - 用矩阵乘法（GEMM）来替代循环卷积操作，以加速计算。\n",
    "      - 这要求我们把输入图像中每个卷积核感受野 (receptive field) 的区域展开成一个列向量。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : np.ndarray\n",
    "        输入数据，形状为 (N, C, H, W)\n",
    "        N: 批量大小（batch size）\n",
    "        C: 通道数（如 RGB = 3）\n",
    "        H: 图像高度\n",
    "        W: 图像宽度\n",
    "    filter_h : int\n",
    "        卷积核的高度\n",
    "    filter_w : int\n",
    "        卷积核的宽度\n",
    "    stride : int\n",
    "        卷积滑动的步长\n",
    "    pad : int\n",
    "        图像边缘填充的像素数（padding）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : np.ndarray\n",
    "        展开后的二维矩阵，形状为 (N*out_h*out_w, C*filter_h*filter_w)\n",
    "        每一行对应卷积核在输入图像上一个位置的感受野区域。\n",
    "    \"\"\"\n",
    "    # 1️⃣ 从输入数据中取出基本维度信息\n",
    "    N, C, H, W = input_data.shape\n",
    "\n",
    "    # 2️⃣ 计算卷积输出的特征图尺寸 (输出高度和宽度)\n",
    "    # 卷积计算公式：\n",
    "    # out_h = (H + 2*pad - filter_h) / stride + 1\n",
    "    # out_w = (W + 2*pad - filter_w) / stride + 1\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    # 3️⃣ 在输入图像周围填充0（padding）\n",
    "    # np.pad的参数 [(batch维度), (通道维度), (高维度), (宽维度)]\n",
    "    # 这里在高和宽方向上各填充 pad 个像素\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "\n",
    "    # 4️⃣ 创建一个空矩阵，用于存放所有卷积窗口的区域\n",
    "    # 每个窗口区域的形状是 (C, filter_h, filter_w)\n",
    "    # 输出特征图有 out_h * out_w 个窗口，每个批次 N 个样本\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    # 5️⃣ 遍历卷积核的每个位置，将对应区域取出来放到 col 中\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            # 按步长 stride 取样：相当于卷积核在输入上滑动\n",
    "            # img[..., y:y_max:stride, x:x_max:stride] 表示所有样本在当前滤波位置的值\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    # 6️⃣ 调整维度顺序 (transpose)：\n",
    "    # 目标：把(N, C, filter_h, filter_w, out_h, out_w)\n",
    "    # 调整为 (N*out_h*out_w, C*filter_h*filter_w)\n",
    "    # 这样每一行就是一个感受野区域展开后的向量。\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "\n",
    "    return col\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    将 im2col 展开的二维矩阵重新还原为原始输入图像（或特征图）的形状。\n",
    "    这是卷积层反向传播中常用的操作。\n",
    "\n",
    "    在 forward（前向传播）中：\n",
    "        input (N, C, H, W) -> im2col -> (N*out_h*out_w, C*filter_h*filter_w)\n",
    "    在 backward（反向传播）中：\n",
    "        dcol -> col2im -> (N, C, H, W)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col : np.ndarray\n",
    "        通过 im2col 展开的 2D 矩阵（或它的梯度版本）\n",
    "        形状通常为 (N*out_h*out_w, C*filter_h*filter_w)\n",
    "    input_shape : tuple\n",
    "        原始输入数据的形状，例如 (N, C, H, W)\n",
    "    filter_h : int\n",
    "        卷积核高度\n",
    "    filter_w : int\n",
    "        卷积核宽度\n",
    "    stride : int\n",
    "        卷积的步长\n",
    "    pad : int\n",
    "        填充的像素数\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img : np.ndarray\n",
    "        还原后的四维张量 (N, C, H, W)\n",
    "    \"\"\"\n",
    "    # 1️⃣ 解包输入形状\n",
    "    N, C, H, W = input_shape\n",
    "\n",
    "    # 2️⃣ 计算卷积输出的空间尺寸\n",
    "    # 与 im2col 相同的公式\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    # 3️⃣ 把 col 恢复成六维张量的形式：\n",
    "    # (N, out_h, out_w, C, filter_h, filter_w)\n",
    "    # 然后转置为 (N, C, filter_h, filter_w, out_h, out_w)\n",
    "    # 这一步相当于还原 im2col 展开前的结构\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    # 4️⃣ 创建一个全零的图像张量（包含padding）\n",
    "    # 注意尺寸：加上 2*pad 的边界，以及 stride-1 的余量，保证索引不越界\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "\n",
    "    # 5️⃣ 将每个卷积窗口（col中的值）叠加回 img 中对应的位置\n",
    "    # 因为在卷积过程中，不同窗口的感受野可能会重叠，\n",
    "    # 所以在还原时需要“加起来”而不是直接覆盖。\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            # += 的原因：不同卷积窗口对同一个输入像素可能有贡献（重叠部分）\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    # 6️⃣ 去掉 padding 部分，恢复原始输入大小\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W              # 卷积核（权重），形状: (滤波器数量, 通道数, 高, 宽)\n",
    "        self.b = b              # 偏置项\n",
    "        self.stride = stride    # 步幅（每次移动的像素数）\n",
    "        self.pad = pad          # 填充（在边缘补0）\n",
    "        \n",
    "        # 这些变量会在 forward 中保存中间结果，用于 backward（反向传播）\n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 存储梯度（在反向传播时计算）\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape   # 卷积核形状\n",
    "        N, C, H, W = x.shape           # 输入数据形状 (batch数, 通道, 高, 宽)\n",
    "\n",
    "        # 输出特征图的尺寸计算公式\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        # 将输入图像展开成二维矩阵（每个滤波区域一行）\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        # 将卷积核展开成二维矩阵（每个滤波器一列）\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        # 矩阵乘法实现卷积（比循环快得多）\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        # 将输出重新变回 (N, 通道数, 高, 宽)\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        # 保存中间结果以便反向传播使用\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "    def backward(self, dout):\n",
    "        # ============================================================\n",
    "        # 1️⃣ 取出权重形状信息\n",
    "        # ============================================================\n",
    "        # FN: 卷积核（滤波器）的数量，对应输出通道数\n",
    "        # C : 输入的通道数（例如 RGB 图像为 3）\n",
    "        # FH, FW: 卷积核的高和宽\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 2️⃣ 调整 dout 的维度以便矩阵运算\n",
    "        # ============================================================\n",
    "        # dout 是来自上层的梯度，形状为 (N, FN, out_h, out_w)\n",
    "        # 但在 forward 时，我们将输入通过 im2col() 展开成二维矩阵，\n",
    "        # 因此为了对齐矩阵乘法的维度，这里也需要将 dout 展平。\n",
    "        #\n",
    "        # 具体操作：\n",
    "        # - 先把通道 FN 移到最后 (N, out_h, out_w, FN)\n",
    "        # - 再把整个输出展平成二维矩阵 (N*out_h*out_w, FN)\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "        # 此时每一行对应一个卷积窗口的输出梯度向量（长度 = 滤波器个数 FN）。\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 3️⃣ 计算偏置项的梯度 self.db\n",
    "        # ============================================================\n",
    "        # 每个卷积核都有一个偏置项 b_j，它被加到该通道所有位置上。\n",
    "        # 因此偏置的梯度就是所有样本、所有空间位置的梯度求和。\n",
    "        #\n",
    "        # 数学表达式：\n",
    "        # db_j = Σ_{n,h,w} dout[n, j, h, w]\n",
    "        self.db = np.sum(dout, axis=0)  # 对所有样本求和，axis=0 表示按列求和\n",
    "        # 结果形状为 (FN,)，即每个卷积核一个偏置梯度。\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 4️⃣ 计算卷积核权重的梯度 self.dW\n",
    "        # ============================================================\n",
    "        # 在前向传播中，卷积计算可以表示为：\n",
    "        #     out = col @ col_W + b\n",
    "        # 其中：\n",
    "        #     col     → 输入展开矩阵，形状 (N*out_h*out_w, C*FH*FW)\n",
    "        #     col_W   → 卷积核展开矩阵，形状 (C*FH*FW, FN)\n",
    "        #\n",
    "        # 反向传播中，根据链式法则：\n",
    "        #     dW = col^T @ dout\n",
    "        # 这表示每个权重的梯度由输入与输出梯度共同决定。\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        # 上式结果形状为 (C*FH*FW, FN)，与 col_W 相同。\n",
    "        #\n",
    "        # 接着还原回卷积核的原始形状 (FN, C, FH, FW)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        # 现在 self.dW[i] 表示第 i 个卷积核的梯度矩阵。\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 5️⃣ 计算输入展开矩阵的梯度 dcol\n",
    "        # ============================================================\n",
    "        # 在 forward 中：\n",
    "        #     out = col @ col_W\n",
    "        # 所以在 backward 时：\n",
    "        #     dcol = dout @ col_W^T\n",
    "        # 表示上层误差反向传播回输入展开矩阵的梯度。\n",
    "        #\n",
    "        # 形状对齐：\n",
    "        #     dout.shape   = (N*out_h*out_w, FN)\n",
    "        #     col_W.T.shape = (FN, C*FH*FW)\n",
    "        #     → 结果 dcol.shape = (N*out_h*out_w, C*FH*FW)\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 6️⃣ 将展开后的输入梯度还原回输入形状\n",
    "        # ============================================================\n",
    "        # 在 forward 时，我们使用 im2col 将输入的局部块展开为行；\n",
    "        # 在 backward 时，要做逆操作，把这些梯度重新“拼”回输入图像。\n",
    "        #\n",
    "        # 注意：卷积窗口之间有重叠区域，col2im() 会自动将这些重叠位置的梯度相加。\n",
    "        #\n",
    "        # 输入参数：\n",
    "        #   dcol        → 展开的输入梯度矩阵\n",
    "        #   self.x.shape → 原输入形状 (N, C, H, W)\n",
    "        #   FH, FW      → 卷积核大小\n",
    "        #   stride, pad → 步幅和填充，与 forward 时一致\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        # dx 即为最终传递给前一层的输入梯度。\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 7️⃣ 返回输入梯度\n",
    "        # ============================================================\n",
    "        # 这样上一层就可以继续接收梯度，完成整个网络的反向传播。\n",
    "        return dx\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        # ============================================================\n",
    "        # 初始化池化层的超参数\n",
    "        # ============================================================\n",
    "        # pool_h, pool_w : 池化窗口的高和宽（例如 2x2 池化）\n",
    "        # stride          : 池化的步幅（窗口每次移动的距离）\n",
    "        # pad             : 填充的像素数，通常池化不使用填充（pad=0）\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 在前向传播中保存的中间变量（供反向传播使用）\n",
    "        self.x = None        # 保存输入数据\n",
    "        self.arg_max = None  # 记录每个池化区域中最大值的位置（反向传播用）\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 前向传播（forward）\n",
    "    # ============================================================\n",
    "    def forward(self, x):\n",
    "        # x 的形状： (N, C, H, W)\n",
    "        # N: 批大小(batch size)\n",
    "        # C: 通道数\n",
    "        # H, W: 输入图像的高和宽\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        # 计算输出特征图的高和宽\n",
    "        # 池化层与卷积层类似，也有输出尺寸计算公式：\n",
    "        # out_h = (输入高 - 池化高) / 步幅 + 1\n",
    "        # out_w = (输入宽 - 池化宽) / 步幅 + 1\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        # 使用 im2col 将输入的所有池化窗口展开成二维矩阵\n",
    "        # 每一行代表一个池化区域（例如 2x2 → 长度4的一行）\n",
    "        # 展开后形状: (N*out_h*out_w*C, pool_h*pool_w)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "\n",
    "        # 将每个池化窗口展开后重新整理形状\n",
    "        # 把每个池化窗口的元素拉成一行方便取最大值\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        # 对每一行取最大值所在的索引位置（用于反向传播）\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "\n",
    "        # 对每一行取最大值（即池化操作）\n",
    "        out = np.max(col, axis=1)\n",
    "\n",
    "        # 重新变回 (N, C, out_h, out_w) 的形状\n",
    "        # 注意这里先 reshape 再 transpose，是因为 im2col 展开时的顺序\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        # 保存中间变量，反向传播时需要用\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        # 返回池化结果\n",
    "        return out\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 反向传播（backward）\n",
    "    # ============================================================\n",
    "    def backward(self, dout):\n",
    "        # dout 是上层传下来的梯度，形状为 (N, C, out_h, out_w)\n",
    "        # 池化层反向传播的核心思想：\n",
    "        #   前向时最大值“获胜”，反向时只有最大值位置能收到梯度，\n",
    "        #   其他位置的梯度为 0。\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        # 现在形状为 (N, out_h, out_w, C)\n",
    "        # 这样方便按输出位置对应地展开为矩阵形式。\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 1️⃣ 计算每个池化区域的大小\n",
    "        # ============================================================\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 2️⃣ 创建与展开后的池化窗口对应的梯度矩阵\n",
    "        # ============================================================\n",
    "        # 目标：为每个池化区域分配梯度\n",
    "        #\n",
    "        # dout.size 表示总的输出元素数量 = N*out_h*out_w*C\n",
    "        # 因此这里创建一个形状为 (dout.size, pool_size) 的全零矩阵，\n",
    "        # 每一行对应一个池化区域（与 forward 展开的行一一对应）。\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "\n",
    "        # 将上层的梯度填充到最大值位置：\n",
    "        #   arg_max 记录了每个池化区域最大值的索引\n",
    "        #   dout.flatten() 展开成一维\n",
    "        #   np.arange(self.arg_max.size) 生成行索引\n",
    "        #\n",
    "        # 效果：\n",
    "        #  只有最大值对应的位置接收梯度，其他位置梯度为 0\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "\n",
    "        # 将 dmax 的形状还原为与前向展开匹配的形状\n",
    "        # 形状为 (N, out_h, out_w, C, pool_size)\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 3️⃣ 将展开后的梯度矩阵再 reshape 为 (行, 列) 形式\n",
    "        # ============================================================\n",
    "        # 这一步与 forward 中的 im2col 对应。\n",
    "        # 把所有池化窗口的梯度重新排列成二维矩阵，准备 col2im 还原。\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 4️⃣ 将展开的梯度矩阵还原回输入形状\n",
    "        # ============================================================\n",
    "        # col2im 的作用：把每个窗口的梯度“放回”输入图像对应的位置。\n",
    "        # 对于重叠的区域，col2im 会自动将梯度相加。\n",
    "        #\n",
    "        # 参数：\n",
    "        #   dcol: 展开的梯度矩阵\n",
    "        #   self.x.shape: 原输入形状 (N, C, H, W)\n",
    "        #   self.pool_h, self.pool_w: 池化窗口大小\n",
    "        #   stride, pad: 与 forward 相同\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "\n",
    "        # ============================================================\n",
    "        # 5️⃣ 返回输入梯度 dx\n",
    "        # ============================================================\n",
    "        # dx 传递给前一层，用于继续反向传播。\n",
    "        return dx\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
